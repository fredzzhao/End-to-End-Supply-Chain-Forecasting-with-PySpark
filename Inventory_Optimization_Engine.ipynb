{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Environment Setup"
      ],
      "metadata": {
        "id": "wlhT27f2n3LT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "\n",
        "try:\n",
        "    java_path = subprocess.check_output([\"which\", \"java\"]).decode(\"utf-8\").strip()\n",
        "    if os.path.islink(java_path):\n",
        "        java_path = os.path.realpath(java_path)\n",
        "    java_home = os.path.dirname(os.path.dirname(java_path))\n",
        "\n",
        "    print(f\"Detected Java Home: {java_home}\")\n",
        "    os.environ[\"JAVA_HOME\"] = java_home\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Failed to detect Java: {e}\")\n",
        "    os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
        "\n",
        "# Install PySpark & Findspark\n",
        "!pip install pyspark findspark -q\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "else:\n",
        "    print(\"Drive already mounted.\")\n",
        "\n",
        "# Initialize Spark\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"SupplyChain_Forecast_Auto\") \\\n",
        "    .config(\"spark.ui.port\", \"4050\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"PySpark Initialized Successfully!\")\n",
        "print(f\"Spark Version: {spark.version}\")\n",
        "print(\"=\"*30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBVwgHS-03nw",
        "outputId": "56c7719e-a9a0-4013-e9fb-06e9eb0a677f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected Java Home: /usr/lib/jvm/java-17-openjdk-amd64\n",
            "Mounted at /content/drive\n",
            "\n",
            "==============================\n",
            "PySpark Initialized Successfully!\n",
            "Spark Version: 4.0.1\n",
            "==============================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Data Ingestion"
      ],
      "metadata": {
        "id": "uXduh6gDnz6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: Update 'file_path' to point to your local dataset or mounted drive\n",
        "file_path = \"/content/drive/MyDrive/Colab Notebooks/Data/train.csv\"\n",
        "\n",
        "# Read CSV with schema inference\n",
        "# inferSchema=True allows Spark to guess data types (e.g., Integer, String)\n",
        "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "\n",
        "print(\"Data Loaded Successfully\")\n",
        "print(f\"Total Records: {df.count()}\")\n",
        "print(f\"Total Columns: {len(df.columns)}\")\n",
        "\n",
        "print(\"\\n--- Raw Data Schema ---\")\n",
        "df.printSchema()\n",
        "\n",
        "print(\"\\n--- Data Preview (Top 3 Rows) ---\")\n",
        "df.show(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ynws9BeW2MhR",
        "outputId": "b3fd15be-4c5d-44e3-ce80-cca929ae0788"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Loaded Successfully\n",
            "Total Records: 9800\n",
            "Total Columns: 18\n",
            "\n",
            "--- Raw Data Schema ---\n",
            "root\n",
            " |-- Row ID: integer (nullable = true)\n",
            " |-- Order ID: string (nullable = true)\n",
            " |-- Order Date: string (nullable = true)\n",
            " |-- Ship Date: string (nullable = true)\n",
            " |-- Ship Mode: string (nullable = true)\n",
            " |-- Customer ID: string (nullable = true)\n",
            " |-- Customer Name: string (nullable = true)\n",
            " |-- Segment: string (nullable = true)\n",
            " |-- Country: string (nullable = true)\n",
            " |-- City: string (nullable = true)\n",
            " |-- State: string (nullable = true)\n",
            " |-- Postal Code: integer (nullable = true)\n",
            " |-- Region: string (nullable = true)\n",
            " |-- Product ID: string (nullable = true)\n",
            " |-- Category: string (nullable = true)\n",
            " |-- Sub-Category: string (nullable = true)\n",
            " |-- Product Name: string (nullable = true)\n",
            " |-- Sales: string (nullable = true)\n",
            "\n",
            "\n",
            "--- Data Preview (Top 3 Rows) ---\n",
            "+------+--------------+----------+----------+------------+-----------+---------------+---------+-------------+-----------+----------+-----------+------+---------------+---------------+------------+--------------------+------+\n",
            "|Row ID|      Order ID|Order Date| Ship Date|   Ship Mode|Customer ID|  Customer Name|  Segment|      Country|       City|     State|Postal Code|Region|     Product ID|       Category|Sub-Category|        Product Name| Sales|\n",
            "+------+--------------+----------+----------+------------+-----------+---------------+---------+-------------+-----------+----------+-----------+------+---------------+---------------+------------+--------------------+------+\n",
            "|     1|CA-2017-152156|08/11/2017|11/11/2017|Second Class|   CG-12520|    Claire Gute| Consumer|United States|  Henderson|  Kentucky|      42420| South|FUR-BO-10001798|      Furniture|   Bookcases|Bush Somerset Col...|261.96|\n",
            "|     2|CA-2017-152156|08/11/2017|11/11/2017|Second Class|   CG-12520|    Claire Gute| Consumer|United States|  Henderson|  Kentucky|      42420| South|FUR-CH-10000454|      Furniture|      Chairs|Hon Deluxe Fabric...|731.94|\n",
            "|     3|CA-2017-138688|12/06/2017|16/06/2017|Second Class|   DV-13045|Darrin Van Huff|Corporate|United States|Los Angeles|California|      90036|  West|OFF-LA-10000240|Office Supplies|      Labels|Self-Adhesive Add...| 14.62|\n",
            "+------+--------------+----------+----------+------------+-----------+---------------+---------+-------------+-----------+----------+-----------+------+---------------+---------------+------------+--------------------+------+\n",
            "only showing top 3 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Data Cleaning"
      ],
      "metadata": {
        "id": "4NZ0pnt-N13i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, to_date, regexp_replace\n",
        "from pyspark.sql.types import DoubleType\n",
        "\n",
        "# Disable ANSI Strict Mode\n",
        "# Ensure Spark returns NULL instead of throwing exceptions for malformed data\n",
        "spark.conf.set(\"spark.sql.ansi.enabled\", \"false\")\n",
        "\n",
        "# Define expected date format (DD/MM/YYYY based on raw data inspection)\n",
        "date_fmt = 'dd/MM/yyyy'\n",
        "\n",
        "# Cleaning Logic\n",
        "# 1. Sales: Remove '$' and ',' characters, then cast to Double\n",
        "# 2. Order Date: Parse string to DateType\n",
        "df_cleaned = df.withColumn(\"Sales\",\n",
        "                           regexp_replace(col(\"Sales\"), \"[$,]\", \"\")\n",
        "                           .cast(DoubleType())) \\\n",
        "               .withColumn(\"Order Date\",\n",
        "                           to_date(col(\"Order Date\"), date_fmt))\n",
        "\n",
        "# Data Quality Check\n",
        "total_count = df.count()\n",
        "null_sales = df_cleaned.filter(col(\"Sales\").isNull()).count()\n",
        "null_date = df_cleaned.filter(col(\"Order Date\").isNull()).count()\n",
        "\n",
        "print(f\"\\n--- Data Quality Report ---\")\n",
        "print(f\"Total Rows Processed: {total_count}\")\n",
        "print(f\"Invalid Sales Records (Dropped): {null_sales}\")\n",
        "print(f\"Invalid Date Records: {null_date}\")\n",
        "\n",
        "# Drop Malformed Records\n",
        "# Remove rows where critical business fields are invalid\n",
        "df_final = df_cleaned.dropna(subset=[\"Sales\", \"Order Date\"])\n",
        "print(f\"Final Cleaned Row Count: {df_final.count()}\")\n",
        "\n",
        "df_final.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A04oTn0s4Nsc",
        "outputId": "9df3c984-d459-4f83-f09d-76c0a2ec8d3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Data Quality Report ---\n",
            "Total Rows Processed: 9800\n",
            "Invalid Sales Records (Dropped): 292\n",
            "Invalid Date Records: 0\n",
            "Final Cleaned Row Count: 9508\n",
            "+------+--------------+----------+----------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+--------+\n",
            "|Row ID|      Order ID|Order Date| Ship Date|     Ship Mode|Customer ID|  Customer Name|  Segment|      Country|           City|     State|Postal Code|Region|     Product ID|       Category|Sub-Category|        Product Name|   Sales|\n",
            "+------+--------------+----------+----------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+--------+\n",
            "|     1|CA-2017-152156|2017-11-08|11/11/2017|  Second Class|   CG-12520|    Claire Gute| Consumer|United States|      Henderson|  Kentucky|      42420| South|FUR-BO-10001798|      Furniture|   Bookcases|Bush Somerset Col...|  261.96|\n",
            "|     2|CA-2017-152156|2017-11-08|11/11/2017|  Second Class|   CG-12520|    Claire Gute| Consumer|United States|      Henderson|  Kentucky|      42420| South|FUR-CH-10000454|      Furniture|      Chairs|Hon Deluxe Fabric...|  731.94|\n",
            "|     3|CA-2017-138688|2017-06-12|16/06/2017|  Second Class|   DV-13045|Darrin Van Huff|Corporate|United States|    Los Angeles|California|      90036|  West|OFF-LA-10000240|Office Supplies|      Labels|Self-Adhesive Add...|   14.62|\n",
            "|     4|US-2016-108966|2016-10-11|18/10/2016|Standard Class|   SO-20335| Sean O'Donnell| Consumer|United States|Fort Lauderdale|   Florida|      33311| South|FUR-TA-10000577|      Furniture|      Tables|Bretford CR4500 S...|957.5775|\n",
            "|     5|US-2016-108966|2016-10-11|18/10/2016|Standard Class|   SO-20335| Sean O'Donnell| Consumer|United States|Fort Lauderdale|   Florida|      33311| South|OFF-ST-10000760|Office Supplies|     Storage|Eldon Fold 'N Rol...|  22.368|\n",
            "+------+--------------+----------+----------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+--------+\n",
            "only showing top 5 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Feature Engineering"
      ],
      "metadata": {
        "id": "HqNLVPQSPFMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import year, weekofyear, sum as _sum, avg, col, lag\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# 1. Aggregation: Order Level -> Weekly Category Level\n",
        "# Aggregate data to \"Weekly Sales per Sub-Category\" to reduce sparsity\n",
        "df_weekly = df_final.withColumn(\"Year\", year(\"Order Date\")) \\\n",
        "                    .withColumn(\"Week\", weekofyear(\"Order Date\")) \\\n",
        "                    .groupBy(\"Year\", \"Week\", \"Sub-Category\") \\\n",
        "                    .agg(_sum(\"Sales\").alias(\"Weekly_Sales\"))\n",
        "\n",
        "# 2. Window Definition for Time-Series Features\n",
        "# Partition by Product Category, Ordered by Time\n",
        "window_spec = Window.partitionBy(\"Sub-Category\").orderBy(\"Year\", \"Week\")\n",
        "\n",
        "# 3. Feature Generation (Lag & Rolling Metrics)\n",
        "# Lag_1_Week: Sales from the previous week (Autocorrelation)\n",
        "# Rolling_Avg_4_Weeks: Trend indicator over the last month\n",
        "df_features = df_weekly.withColumn(\"Lag_1_Week\", lag(\"Weekly_Sales\", 1).over(window_spec)) \\\n",
        "                       .withColumn(\"Lag_4_Weeks\", lag(\"Weekly_Sales\", 4).over(window_spec)) \\\n",
        "                       .withColumn(\"Rolling_Avg_4_Weeks\", avg(\"Weekly_Sales\").over(window_spec.rowsBetween(-3, 0)))\n",
        "\n",
        "# 4. Drop Nulls Created by Lag\n",
        "df_model_data = df_features.dropna()\n",
        "\n",
        "print(\"Feature Engineering Complete\")\n",
        "df_model_data.show(5)"
      ],
      "metadata": {
        "id": "7Z37g8oc5hJn",
        "outputId": "704eecad-1a1a-4ed7-c66b-0d147d97e757",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Engineering Complete\n",
            "+----+----+------------+------------------+------------------+------------------+-------------------+\n",
            "|Year|Week|Sub-Category|      Weekly_Sales|        Lag_1_Week|       Lag_4_Weeks|Rolling_Avg_4_Weeks|\n",
            "+----+----+------------+------------------+------------------+------------------+-------------------+\n",
            "|2015|   7| Accessories|474.41999999999996|            115.36|              31.2|           463.8425|\n",
            "|2015|   8| Accessories|             62.31|474.41999999999996|            796.69| 280.24749999999995|\n",
            "|2015|  10| Accessories|            479.97|             62.31|             468.9|            283.015|\n",
            "|2015|  11| Accessories|115.75999999999999|            479.97|            115.36|            283.115|\n",
            "|2015|  12| Accessories|            170.24|115.75999999999999|474.41999999999996|             207.07|\n",
            "+----+----+------------+------------------+------------------+------------------+-------------------+\n",
            "only showing top 5 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Modeling"
      ],
      "metadata": {
        "id": "ux3j9mXXPKrI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "# 1. Vectorization\n",
        "# Assemble input features into a single vector column for Spark ML\n",
        "feature_cols = [\"Week\", \"Lag_1_Week\", \"Lag_4_Weeks\", \"Rolling_Avg_4_Weeks\"]\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "df_final_prep = assembler.transform(df_model_data)\n",
        "\n",
        "# 2. Time-Based Train/Test Split\n",
        "# Use time-based split instead of random split to prevent data leakage\n",
        "split_year = 2017\n",
        "train_data = df_final_prep.filter(col(\"Year\") < split_year)\n",
        "test_data = df_final_prep.filter(col(\"Year\") >= split_year)\n",
        "\n",
        "print(f\"Training Records (Pre-{split_year}): {train_data.count()}\")\n",
        "print(f\"Test Records ({split_year}+): {test_data.count()}\")\n",
        "\n",
        "# 3. Model Training\n",
        "# RandomForest is selected for its ability to handle non-linear relationships\n",
        "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"Weekly_Sales\", numTrees=100)\n",
        "model = rf.fit(train_data)\n",
        "\n",
        "# 4. Prediction & Evaluation\n",
        "predictions = model.transform(test_data)\n",
        "evaluator = RegressionEvaluator(labelCol=\"Weekly_Sales\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(f\"Model Performance (RMSE): {rmse:.2f}\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "predictions.select(\"Year\", \"Week\", \"Sub-Category\", \"Weekly_Sales\", \"prediction\").show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WPnnKHuW_z0",
        "outputId": "2fefdfae-8832-4c90-9714-f5e39091228a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Records (Pre-2017): 1200\n",
            "Test Records (2017+): 1413\n",
            "\n",
            "========================================\n",
            "Model Performance (RMSE): 1164.46\n",
            "========================================\n",
            "+----+----+------------+------------------+------------------+\n",
            "|Year|Week|Sub-Category|      Weekly_Sales|        prediction|\n",
            "+----+----+------------+------------------+------------------+\n",
            "|2017|   1| Accessories|387.15200000000004|2716.7355204159117|\n",
            "|2017|   2| Accessories|             674.9|402.74854129549954|\n",
            "|2017|   3| Accessories|           863.706|511.50262330915444|\n",
            "|2017|   4| Accessories|             99.98|374.82522257151754|\n",
            "|2017|   6| Accessories|221.82999999999998|  541.020372455817|\n",
            "+----+----+------------+------------------+------------------+\n",
            "only showing top 5 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Inventory Optimization"
      ],
      "metadata": {
        "id": "Ei9M4nosPyeC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sqrt, mean, round, lit\n",
        "\n",
        "# 1. Calculate Category-Specific Error (RMSE)\n",
        "# Different categories have different volatilities\n",
        "df_with_error = predictions.withColumn(\"Error\", col(\"Weekly_Sales\") - col(\"prediction\"))\n",
        "\n",
        "category_stats = df_with_error.groupBy(\"Sub-Category\").agg(\n",
        "    (sqrt(mean(col(\"Error\")**2))).alias(\"Category_RMSE\")\n",
        ")\n",
        "\n",
        "# 2. Inventory Policy Definition\n",
        "# Target Service Level: 95% (Z-Score approx. 1.65)\n",
        "Z_SCORE = 1.65\n",
        "\n",
        "# 3. Generate Replenishment Plan\n",
        "# Total Inventory = Cycle Stock (Predicted Demand) + Safety Stock (Buffer)\n",
        "df_inventory = predictions.join(category_stats, \"Sub-Category\", \"left\")\n",
        "\n",
        "df_final_plan = df_inventory.withColumn(\"Safety_Stock\", round(col(\"Category_RMSE\") * Z_SCORE, 0)) \\\n",
        "                            .withColumn(\"Cycle_Stock\", round(col(\"prediction\"), 0)) \\\n",
        "                            .withColumn(\"Total_Inventory_Needed\", col(\"Cycle_Stock\") + col(\"Safety_Stock\"))\n",
        "\n",
        "print(\"=== Final Inventory Recommendations (Top Needs) ===\")\n",
        "df_final_plan.select(\"Year\", \"Week\", \"Sub-Category\",\n",
        "                     \"Cycle_Stock\", \"Safety_Stock\", \"Total_Inventory_Needed\") \\\n",
        "             .orderBy(col(\"Total_Inventory_Needed\").desc()) \\\n",
        "             .show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5SvL8OrX8TW",
        "outputId": "146fc00d-defb-411e-b787-03cb99042822"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Final Inventory Recommendations (Top Needs) ===\n",
            "+----+----+------------+-----------+------------+----------------------+\n",
            "|Year|Week|Sub-Category|Cycle_Stock|Safety_Stock|Total_Inventory_Needed|\n",
            "+----+----+------------+-----------+------------+----------------------+\n",
            "|2018|  30|     Copiers|     4337.0|      8235.0|               12572.0|\n",
            "|2018|  46|     Copiers|     4153.0|      8235.0|               12388.0|\n",
            "|2017|  26|     Copiers|     4137.0|      8235.0|               12372.0|\n",
            "|2017|  21|     Copiers|     4088.0|      8235.0|               12323.0|\n",
            "|2018|  12|     Copiers|     3980.0|      8235.0|               12215.0|\n",
            "|2017|  49|     Copiers|     3912.0|      8235.0|               12147.0|\n",
            "|2017|  51|     Copiers|     3839.0|      8235.0|               12074.0|\n",
            "|2018|  52|     Copiers|     3779.0|      8235.0|               12014.0|\n",
            "|2018|  44|     Copiers|     3488.0|      8235.0|               11723.0|\n",
            "|2018|  19|     Copiers|     3482.0|      8235.0|               11717.0|\n",
            "+----+----+------------+-----------+------------+----------------------+\n",
            "only showing top 10 rows\n"
          ]
        }
      ]
    }
  ]
}